\documentclass[numbers]{proposalnsf}

\usepackage{code}

\title{Collaborative Research: CIRC: New: Medium: A Development and Experimental Environment for Meta-Fuzzing} 
\author{Alex Groce, Claire Le Goues, etc.}
\date{August 2023}

\newcommand{\um}{\texttt{universalmutator}}

\begin{document}

\section*{Collaborative Research: CIRC: New: Medium: A Development and Experimental Environment for Meta-Fuzzing}

\subsection*{Overview}
\vspace{-2mm}


The complexity of modern software systems, and the need for widely used libraries and other ``code infrastructure'' to be absolutely reliable, demands effective \emph{testing}.  Bugs in code have increasing impact on society at large (e.g., numerous security breaches traceable to incorrect code, and very high economic cost.  Testing remains to this day the single most effective means of finding bugs when their cost is low.   Moreover, fuzzing, originally limited largely to the software security world, is the most promising new approach to massively automated and effective testing of software.   However, to date most fuzzing research, even if theoretically able to generalize to multiple underlying fuzzers, has been focused on \emph{producing a new fuzzer}.  That is, if a researcher (or security developer at, e.g., Google) devises a way to improve the effectiveness of fuzzing, the common practice is to implement a new, competing fuzzer, or modify a well-know existing fuzzer.  In some cases, this is appropriate: the approach defines a fuzzing technique.  However, the most promising fuzzing advances likely are \emph{meta-fuzzing} approaches that improve the performance, potentially, of \emph{all} fuzzers, without requiring modification of the underlying fuzzer itself.  Such techniques primarily include those that modify the source or binary of the fuzzing target to help the fuzzer, and techniques that make use of multiple fuzzers as ``black boxes'' that contribute to an ensemble fuzzing effort.  This proposal addresses the serious under-examination of such techniques, by aiming to construct a framework for development of meta-fuzzing approaches of these categories, making it easy to apply a technique to multiple fuzzers, and making it easy to evaluate/benchmark such techniques.  The ability to evaluate methods is necessary to enable ensemble methods that weight fuzzers by performance, and so is required for a meta-fuzzing development platform, in any case.

\subsection*{Intellectual Merit} 
\vspace{-2mm}
The importance of fuzzing to future efforts to find subtle bugs in complex software systems is well-known.  This proposal aims primarily to boost research and development in the area of fuzzing methods that can work \emph{for any fuzzer}, including future, better, underlying fuzzing algorithms.  As part of that work, it also aims to standardize and advance the evaluation of fuzzing algorithms, at present a major bottleneck in fuzzing science advances.  We further expect meta-fuzzing approaches to be particular effective in improving understanding of general program dynamics, rather than tool-engineering-oriented fuzzing techniques.
\subsection*{Broader Impacts}
\vspace{-2mm}

Correct software is increasingly important in our modern digital/networked society since ``software is eating the world'' and this process shows no sign of stopping or even decelerating. 
Our project will benefit every area of society where software is employed since our project aims to improve a fundamental method for effective software testing.
We expect that software security in particular will be advanced by the improvements made to the performance of fuzzing methods, and that meta-fuzzing may also help support the emerging focus on applying fuzzing to more functional properties of code.

\paragraph{Keywords:}
CISE; fuzzing, meta-fuzzing, ensemble methods, program transformations



\pagenumbering{gobble}
\newpage  
%\pagenumbering{arabic}
\pagenumbering{gobble}

\section{Introduction}
\label{sec:intro}

Fuzzing \cite{fuzzoverview} is one of the most important recent advances in automated software testing.  Modern coverage-driven fuzzers, such as AFL or libFuzzer, generally work by maintaing a ``corpus'' of potentially interesting inputs for a program (the ``fuzzing target'').  The corpus may consist of a trivial input, or examples of real-world inputs (or test cases), initially.  The fuzzer then selects one such input, modifies it in some fashion, and submits the input to an instrumented version of the program being fuzzed.  If the execution produces novel behavior (e.g., covers new code or takes a new path through already-covered code), the modified input is added to the set of ``interesting'' inputs.   If the execution reveals a bug, of course, it is saved for inspection.   Fuzzers differ widely in their strategies for selecting among interesting inputs, modifying inputs, and determining what constitutes ``new'' behavior worth further exploring, but this broad picture of the basic structure of modern, effective fuzzers, is largely applicable, even to fuzzers making use of machine learning or symbolic execution techniques ``under the hood.''

This basic approach has proved extremely effective in finding bugs, and in the shape of OSSFuzz (supported by Google, the Core Infrastructure Initiative and the OpenSSF), is used to probe a large number of critical open source systems, finding over 8,900 vulnerabilities and 28,000 bugs across 850 projects, to date.

Fuzzing has unsurprisingly become a major topic of academic security and testing research, as a result of the obvious power and success of the basic technique.  However, most such research results in \emph{the development of a new fuzzer.}  The classic structure of a ``fuzzing paper'' is evaluation of a novel fuzzer developed by the academic researchers against a set of well-known fuzzers and recently published academic fuzzers.  Such work often results in the availability of new, powerful fuzzers, of course.  However, in some cases the underlying idea is a relatively isolated concept, that ends up being embedded in some cases in a technically less-than-sophisticated fuzzer.  E.g., the first release of the highly successful Eclipser fuzzer was very successful, due to the power of the lightweight ``symbolic-like'' method used to modify inputs in intelligent ways.  However, the research team acknowledged that their implementation of more mundane aspects of fuzzing was somewhat ad hoc, and the second version of Eclipser moved to using the widely used AFL fuzzer to support most aspects of fuzzing other than the core innovation.  Moreover, when a fuzzing advance is not adopted by other fuzzer developers, it can become ``stuck'' in an outdated fuzzer.  Fuzzing advances only appear in new fuzzers if future fuzzer developers adopt them as ``standard practice.''  To some extent, efforts such as AFLPlusPlus, a version of AFL incorporating many academic and industrial fuzzing advances, attempt to overcome this problem, and in fact AFLPlusPlus is a notably effective fuzzer.  However, there remain many individual programs where some other fuzzer outperforms AFLPlusPlus, and in fact use of many different, even sub-optimal, fuzzers is likely required for truly effective fuzzing.

\emph{Meta-fuzzing} proposes another approach to improving fuzzing, by \emph{moving the fuzzing technique outside the fuzzer itself.}  The simplest such approaches to consider are ones based on altering the fuzzing target, which is of course common across fuzzers.  A simple example is the idea of decomposing comparisons in a program.  Usually, a fuzzer can only observe if a program takes a given branch or not, at the binary level.  Decomposition breaks down CMP instructions into individual bit-level comparisons (or some other appropriate structuring) so that not only whether a branch was taken is visible, but how ``close'' a branch was to being taken (analogous to a branch distance in search-based testing).  There are various alternative versions of the basic idea, in e.g., Steelix and libFuzzer.  These are usually implemented as modifications to the fuzzer's custom instrumentation.  However, the basic idea can also be implemented by transforming the source code of a program to expose the structure of a comparison.  The DeepState property-driven fuzzing tool does this for its own assertion implementations.  Such a transform might take, e.g.:

\begin{code}

if (x == y)
\end{code}

\noindent where {\tt x} and {\tt y} are integer variables, and rewrite the code as:

\begin{code}
cmp\_x\_y = TRUE;
bool cmp\_bit\_0 = bit(x, 0) == bit(y, 0);
if (cmp\_bit\_0) \{cmp\_x\_y = FALSE;\}
bool cmp\_bit\_1 = bit(x, 1) == bit(y, 0);
if (cmp\_bit\_1) \{cmp\_x\_y = FALSE;\}
$\ldots$
if (cmp\_x\_y)
\end{code}

\noindent which will be highly inefficient, of course, but exposes to the fuzzer when it has ``almost'' solved an equality check.  The advantage of such an approach is that it enables \emph{any fuzzer} to make use of the power of decomposition, even if the fuzzer developers were unaware such a technique existed.

While this particular technique, for efficiency reasons, is likely best implemented inside a fuzzer's instrumentation pass, other meta-fuzzing techniques are truly universal.  E.g., in recent work, PIs Groce and Le Goues proposed fuzzing based on program mutants \cite{}, which modifies a fuzzing target in a large variety of ways.  Results show that this approach, by allowing a fuzzer to explore program branches in non-chronological order, can improve the performance of even state-of-the-art fuzzers such as AFLPlusPlus, as benchmarked by Google's FuzzBench.
  
\paragraph{Ensemble Fuzzing.}

\paragraph{Target Transformations.}

\paragraph{Universalized Custom Mutators.}
   
\paragraph{An Example Meta-Fuzzing Application.} The above summaries introduce broad classes of meta-fuzzing approaches, and we have briefly introduced some general-purpose meta-fuzzing techniques (e.g., the use of program mutants to explore branches non-chronologically).  Meta-fuzzing can also be used for more domain-specific purposes, however.  Consider the problem of testing machine learning (ML) algorithms in general, and specifically neural network implementations.  Using off-the-shelf fuzzers directly on ML systems is usually ineffective, because the behavior of the system is not primarily encoded in the source code of the system, and thus visible to code coverage instrumentation.  Instead, \emph{data} in the form of a neural network, decision tree, etc. determines the system behavior.  The ``code'' to be explored is not visible to traditional compiler instrumentation.  A meta-fuzzing approach to this problem would be to define transformers that understand the nature of a machine learning implementation (the standard libraries used widely to implement ML systems) and use the data encodings to produce \emph{source mirrors} that take inputs to the ML algorithm and produce visible code coverage for a fuzzer.  One aspect of our general meta-fuzzing infrastructure, discussed in more detail below, is additionally to support domain-specific fuzzing that falls outside the locus of ``fuzzing Linux applicaiton binaries'' previously centered in fuzzing benchmarking.


\paragraph{The Problem of Evaluation.} Evaluating fuzzers is, even in the current common setting, where evaluation is of usually of a single ``new'' fuzzer, a complex problem, and many evaluations in the literature are insufficient or even incorrect.  Evaluating meta-fuzzing adds a further quantitative (and to some extent qualitative) element to this problem: rather than comparing a single fuzzer to a set of competing fuzzers across benchmarks, evaluating meta-fuzzing methods invovles evaluating a \emph{set} of fuzzers $F_1 \ldots F_n$ against $F'_1 \ldots F'_n$ (where some meta-fuzzing method has been applied), and determining the degree of improvement or degradation in effectiveness in each case.  This alone would make support for benchmarking and comparing fuzzers an important feature of any framework for meta-fuzzing research and development.  However, as noted above, ensemble methods are likely to also make use of fuzzer evaluations in order to allocate resources.  Therefore a major thrust of this effort is to adapt the aspects of the framework that support ensemble fuzzing to support measurements of fuzzer effectiveness as well.  This feature of course has applications beyond meta-fuzzing methods; traditional fuzzer evaluations can also be expected to benefit from a systematic, multi-measurement framework for running a set of fuzzers on multiple benchmarks.

\subsection{Overview of Proposed Work}

This project proposes, first, to create a reliable infrastructure for executing multiple fuzzers on a set of targets and collecting data from these executions.  Such data includes generated inputs and detected bugs, of course, but also measurements of fuzzer effectiveness, e.g., code coverage of corpuses and incremental code coverage.  This infrastructure will be able to run locally for debugging and quick turnaround, and will support cloud deployment for large-scale experiments. Critically, this infrastructure will be designed to support ensemble fuzzing and complex ensemble strategies, from the ground up; support for exchange of generated inputs and shifts in resource allocation will be built-in, rather than added on, and a high-level declarative language for describing ensemble strategies will allow researchers to explore the space of ensemble methods.

A second key element of the proposed system is the development of tooling for expressing source and binary-level transformations to support meta-fuzzing methods that rely on modifying the target to be fuzzer.  Such methods may focus on making more behavior of a system visible to fuzzers within the paradigm of ``source coverage'' currently supported by essentially all modern fuzzers, or on enhancing oracles to enable all fuzzers to detect larger classes of bugs, or on transformations that target the underlying logic and bottlenecks of fuzzing, such as the non-chronological exploration of branches enabled by using program mutants.  This thrust, again, will focus on providing a way for researchers and developers to easily express such transforms and deploy them across a large set of fuzzers.

Finally, a smaller thrust will focus on allowing researchers and developers to write custom mutators for fuzzers in a fuzzer-independent way, with support for automatically generating wrappers to run these ``universalized'' mutators inside the large set of fuzzers (including AFLPlusPlus and libFuzzer) that support custom mutators.

\subsection{Enabled Research Projects}

\subsection{Team's Qualifications}

\subsection{Intellectual Merit and Broader Impacts}

\section{Key Deficiencies of Existing Infrastructure}

\section{Proposed Infrastructure}

\subsection{Thrust 1: A Foundation for Efficient Fuzzer Execution, Orchestration, and Evaluation}

\subsection{Thrust 2: Expressive Source and Binary-Level Transformations to Enable Meta-Fuzzing}

\subsection{Thrust 3: Universalized Custom Mutators}

\section{Enabled Research Opportunities and Projects}

\section{Project Organization Plan}


\section{Results from Prior NSF Support}




\newpage
%\pagenumbering{roman}
%\setcounter{page}{1} 
%\bibliographystyle{unsrt}
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}