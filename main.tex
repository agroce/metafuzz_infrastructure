\documentclass[numbers]{proposalnsf}

\usepackage{code}
\usepackage{url}
\usepackage{xcolor}


\title{Collaborative Research: CIRC: New: Medium: A Development and 
Experimental Environment for Meta-Fuzzing} 
\author{Alex Groce, Claire Le Goues, etc.}
\date{August 2023}

\newcommand{\um}{\texttt{universalmutator}}
\newcommand{\jdh}[1]{\textcolor{red}{#1}}

\begin{document}

\section*{Collaborative Research: CIRC: New: Medium: A Development and 
Experimental Environment for Meta-Fuzzing}

\subsection*{Overview}
\vspace{-2mm}


The complexity of modern software systems, and the need for widely used 
libraries and other ``code infrastructure'' to be reliable, demands effective 
\emph{testing}.  Bugs in code have increasing impact on society at large (e.g., 
numerous security breaches traceable to incorrect code have very high economic 
cost).  Testing remains to this day the single most effective means of finding 
bugs when their cost is low.   Moreover, fuzzing, originally limited largely to 
the software security world, is the most promising new approach to massively 
automated and effective testing of software.   However, to date most fuzzing 
research, even if theoretically able to generalize to multiple underlying 
fuzzers, has been focused on \emph{producing a new fuzzer}.  That is, if a 
researcher (or security developer at, e.g., Google) devises a way to improve 
the effectiveness of fuzzing, the common practice is to implement a new, 
competing fuzzer, or modify a well-know existing fuzzer.  In some cases, this 
is appropriate: the approach defines a fuzzing technique.  However, the most 
promising fuzzing advances likely are \emph{meta-fuzzing} approaches that 
improve the performance, potentially, of \emph{all} fuzzers, without requiring 
modification of the underlying fuzzer itself.  Such techniques primarily 
include those that modify the source or binary of the fuzzing target to help 
the fuzzer, and techniques that make use of multiple fuzzers as ``black boxes'' 
that contribute to an ensemble fuzzing effort.  This proposal addresses the 
serious under-examination of such techniques, by aiming to construct a 
framework for development of meta-fuzzing approaches of these categories, 
making it easy to apply a technique to multiple fuzzers, and making it easy to 
evaluate/benchmark such techniques.  The ability to evaluate methods is 
necessary to enable ensemble methods that weight fuzzers by performance, and so 
is required for a meta-fuzzing development platform, in any case.

\subsection*{Intellectual Merit} 
\vspace{-2mm}
The importance of fuzzing to future efforts to find subtle bugs in complex 
software systems is well-known.  This proposal aims primarily to boost research 
and development in the area of fuzzing methods that can work \emph{for any 
fuzzer}, including future, better, underlying fuzzing algorithms.  As part of 
that work, it also aims to standardize and advance the evaluation of fuzzing 
algorithms, at present a major bottleneck in fuzzing science advances.  We 
further expect meta-fuzzing approaches to be particular effective in improving 
understanding of general program dynamics, rather than 
tool-engineering-oriented fuzzing techniques.
\subsection*{Broader Impacts}
\vspace{-2mm}

Correct software is increasingly important in our modern digital/networked 
society since ``software is eating the world'' and this process shows no sign 
of stopping or even decelerating. 
Our project will benefit every area of society where software is employed since 
our project aims to improve a fundamental method for effective software testing.
We expect that software security in particular will be advanced by the 
improvements made to the performance of fuzzing methods, and that meta-fuzzing 
may also help support the emerging focus on applying fuzzing to more functional 
properties of code.

\paragraph{Keywords:}
CISE; fuzzing, meta-fuzzing, ensemble methods, program transformations



\pagenumbering{gobble}
\newpage  
%\pagenumbering{arabic}
\pagenumbering{gobble}

\section{Introduction}
\label{sec:intro}

Fuzzing \cite{fuzzoverview} is one of the most important recent advances in 
automated software testing.  Modern coverage-driven fuzzers, such as AFL or 
libFuzzer, generally work by maintaing a ``corpus'' of potentially interesting 
inputs for a program (the ``fuzzing target'').  The corpus may consist of a 
trivial input, or examples of real-world inputs (or test cases), initially.  
The fuzzer then selects one such input, modifies it in some fashion, and 
submits the input to an instrumented version of the program being fuzzed.  If 
the execution produces novel behavior (e.g., covers new code or takes a new 
path through already-covered code), the modified input is added to the set of 
``interesting'' inputs.   If the execution reveals a bug, of course, it is 
saved for inspection.   Fuzzers differ widely in their strategies for selecting 
among interesting inputs, modifying inputs, and determining what constitutes 
``new'' behavior worth further exploring, but this broad picture of the basic 
structure of modern, effective fuzzers, is largely applicable, even to fuzzers 
making use of machine learning or symbolic execution techniques ``under the 
hood.''

This basic approach has proved extremely effective in finding bugs, and in the 
shape of OSSFuzz (supported by Google, the Core Infrastructure Initiative and 
the OpenSSF), is used to probe a large number of critical open source systems, 
finding over 8,900 vulnerabilities and 28,000 bugs across 850 projects, to date.

Fuzzing has unsurprisingly become a major topic of academic security and 
testing research, as a result of the obvious power and success of the basic 
technique.  However, most such research results in \emph{the development of a 
new fuzzer.}  The classic structure of a ``fuzzing paper'' is evaluation of a 
novel fuzzer developed by the academic researchers against a set of well-known 
fuzzers and recently published academic fuzzers.  Such work often results in 
the availability of new, powerful fuzzers, of course.  However, in some cases 
the underlying idea is a relatively isolated concept, that ends up being 
embedded in some cases in a technically less-than-sophisticated fuzzer.  E.g., 
the first release of the highly successful Eclipser fuzzer was very successful, 
due to the power of the lightweight ``symbolic-like'' method used to modify 
inputs in intelligent ways.  However, the research team acknowledged that their 
implementation of more mundane aspects of fuzzing was somewhat ad hoc, and the 
second version of Eclipser moved to using the widely used AFL fuzzer to support 
most aspects of fuzzing other than the core innovation.  Moreover, when a 
fuzzing advance is not adopted by other fuzzer developers, it can become 
``stuck'' in an outdated fuzzer.  Fuzzing advances only appear in new fuzzers 
if future fuzzer developers adopt them as ``standard practice.''  There have 
been some attempts to overcome this problem. For example, 
FuzzFactory~\cite{fuzzfactory} is a meta-fork of AFL which enables researchers 
to instatiate and compose fuzzers with custom feedback functions by only 
instrumenting the target program. Another prominent example is AFLPlusPlus, a 
version of AFL incorporating many academic and industrial fuzzing advances, 
which is a notably effective fuzzer.  However, there remain many individual 
programs where some other fuzzer outperforms AFLPlusPlus, and in fact use of 
many different, even sub-optimal, fuzzers is likely required for truly 
effective fuzzing.

\emph{Meta-fuzzing} proposes another approach to improving fuzzing, by 
\emph{moving the fuzzing technique outside the fuzzer itself.}  The simplest 
such approaches to consider are ones based on altering the fuzzing target, 
which is of course common across fuzzers.  A simple example is the idea of 
decomposing comparisons in a program.  Usually, a fuzzer can only observe if a 
program takes a given branch or not, at the binary level.  Decomposition breaks 
down CMP instructions into individual bit-level comparisons (or some other 
appropriate structuring) so that not only whether a branch was taken is 
visible, but how ``close'' a branch was to being taken (analogous to a branch 
distance in search-based testing).  There are various alternative versions of 
the basic idea, in e.g., Steelix and libFuzzer.  These are usually implemented 
as modifications to the fuzzer's custom instrumentation.  However, the basic 
idea can also be implemented by transforming the source code of a program to 
expose the structure of a comparison.  The DeepState property-driven fuzzing 
tool does this for its own assertion implementations, as does FuzzFactory in 
its \texttt{CMP} domain.  Such a transform might take, e.g.:

\begin{code}

if (x == y)
\end{code}

\noindent where {\tt x} and {\tt y} are integer variables, and rewrite the code 
as:

\begin{code}
cmp\_x\_y = TRUE;
bool cmp\_bit\_0 = bit(x, 0) == bit(y, 0);
if (cmp\_bit\_0) \{cmp\_x\_y = FALSE;\}
bool cmp\_bit\_1 = bit(x, 1) == bit(y, 0);
if (cmp\_bit\_1) \{cmp\_x\_y = FALSE;\}
$\ldots$
if (cmp\_x\_y)
\end{code}

\noindent which will be highly inefficient, of course, but exposes to the 
fuzzer when it has ``almost'' solved an equality check.  The advantage of such 
an approach is that it enables \emph{any fuzzer} to make use of the power of 
decomposition, even if the fuzzer developers were unaware such a technique 
existed. \jdh{Too many ``quoted'' words.  It is starting to feel like quotes 
are used for emphasis.  Try to rephrase to more directly get the meaning.}

While this particular technique, for efficiency reasons, is likely best 
implemented inside a fuzzer's instrumentation pass, other meta-fuzzing 
techniques are truly universal.  E.g., in recent work, PIs Groce and Le Goues 
proposed fuzzing based on program mutants \cite{}, which modifies a fuzzing 
target in a large variety of ways.  Results show that this approach, by 
allowing a fuzzer to explore program branches in non-chronological order, can 
improve the performance of even state-of-the-art fuzzers such as AFLPlusPlus, 
as benchmarked by Google's FuzzBench.

A core tenet of meta-fuzzing is that the fuzzing tool need not be aware of what 
the program transformation is trying to achieve. For example, in the 
FuzzFactory framework~\cite{fuzzfactory}, co-PI Padhye demonstrated that new 
fuzzing applications---such as finding worst-case performance or 
memory-allocation bottlenecks, as well as targeting fuzzing towards specific 
program locations---can be instantiated by only transforming the target program 
and using an API to send domain-specific feedback from program execution in the 
form of key-value maps. Padhye also demonstrated that composing program 
transformations, achieved by combining key-value maps across multiple 
FuzzFatory domains (such as breaking down CMP operations and tracking 
\texttt{malloc}'d memory), can produce results that are better than applying 
each transformation individually.

\paragraph{Ensemble Fuzzing.}   Ensemble fuzzing \cite{chen2019enfuzz} is an 
approach that recognizes the need for
diverse methods for test generation, at least in the context of
fuzzing.   Inspired by ensemble methods in machine learning 
\cite{dietterich2002ensemble},
ensemble fuzzing runs multiple fuzzers, and uses inputs generated by
each fuzzer to ``feed'' other fuzzers, since different fuzzers have widely 
differing strengths and weaknesses. Ensemble fuzzing is, by nature, a 
meta-fuzzing approach, in that it uses individual fuzzers as ``black boxes'' 
that take in inputs and a program and produce novel inputs of interest.  
Ensemble fuzzing is an extremely promising approach, given that in most 
large-scale benchmarks of fuzzers, there are times when the best fuzzers 
overall perform poorly for some targets, and when the worst fuzzers perform 
well.  The future of fuzzing likely lies in ensemble methods, but the 
implementations proposed thus far tend to 1) not be supported long, and soon 
fail to work at all 2) and/or are limited to very simple naive strategies for 
allocating resources and sharing seeds.  This project aims to make it easy to 
experiment with \emph{sophisticated} ensemble fuzzing approaches, without 
worrying about the infrastructure of executing fuzzers or extracting feedback 
to use in resource allocation.

\paragraph{Target Transformations.}  While ensemble fuzzing is ``meta'' in that 
it uses multiple fuzzers largely as black boxes, another approach largely 
dispenses with direct interaction with fuzzers at all.  As the example above 
suggests, one fuzzer-agnostic way to change the behavior of a fuzzer is to 
change the fuzzing target itself.  This is most easily done at the source 
level, in essence producing a (slightly) different program to fuzz.  In some 
cases, though, binary transformation may be required for reasons of efficiency. 
 \jdh{Source is almost always faster than binary transform.... i'm not 
convinced by this reasoning.} Transformations may be used to make aspects of 
program behavior more visible as coverage, as in the simple example, or to 
improve the oracle for fuzzing, transforming more behaviors into fuzzer-visible 
crashes.  More ambitiously, as in the prior work of PIs Groce and Le Goues, a 
transform may change the behavior of fuzzing algorithms.  Fuzzing program 
mutants allows fuzzers to detect ways to cover branches that, without 
transformation, would not have been executed yet, improving fuzzer 
effectiveness dramatically in some cases.

\paragraph{Universalized Custom Mutators.}  One feature supported by several 
widely used fuzzers is the introduction of custom mutators:  user-written 
additional ways to modify inputs to generate new inputs.  While changing the 
mutation strategy of a particular fuzzer can be a highly fuzzer-specific 
method, focusing on some unusual coverage or ``magic byte identification'' 
element of the particular fuzzer, many custom mutators are naturally fuzzer 
agnostic.  For example, PIs Groce and Le Goues introduced custom mutators for 
fuzzing compilers that resulted in the detection of hundreds of subtle compiler 
bugs \cite{CC22}.  These were implemented in a standalone variant of the 
original AFL fuzzer, but would be more widely used and more effective if 
implemented as custom mutators for a variety of fuzzers.  We propose to support 
research along these lines by allowing users to write a single custom mutator 
in a generic byte-based framework, and provide wrappers to integrate such 
mutators into all fuzzers that support custom mutators (including many of the 
most popular and powerful fuzzers).
   
\paragraph{An Example Meta-Fuzzing Application.} The above summaries introduce 
broad classes of meta-fuzzing approaches, and we have briefly introduced some 
general-purpose meta-fuzzing techniques (e.g., the use of program mutants to 
explore branches non-chronologically).  Meta-fuzzing can also be used for more 
domain-specific purposes, however.  Consider the problem of testing machine 
learning (ML) algorithms in general, and specifically neural network 
implementations.  Using off-the-shelf fuzzers directly on ML systems is usually 
ineffective, because the behavior of the system is not primarily encoded in the 
source code of the system, and thus visible to code coverage instrumentation.  
Instead, \emph{data} in the form of a neural network, decision tree, etc. 
determines the system behavior.  The ``code'' to be explored is not visible to 
traditional compiler instrumentation.  A meta-fuzzing approach to this problem 
would be to define transformers that understand the nature of a machine 
learning implementation (the standard libraries used widely to implement ML 
systems) and use the data encodings to produce \emph{source mirrors} that take 
inputs to the ML algorithm and produce visible code coverage for a fuzzer.  One 
aspect of our general meta-fuzzing infrastructure, discussed in more detail 
below, is additionally to support domain-specific fuzzing that falls outside 
the locus of ``fuzzing Linux application binaries'' previously centered in 
fuzzing benchmarking.


\paragraph{The Problem of Evaluation.} Evaluating fuzzers is, even in the 
current common setting, where evaluation is usually of a single ``new'' fuzzer, 
a complex problem. Many evaluations in the literature are insufficient or even 
incorrect.  Evaluating meta-fuzzing adds a further quantitative (and to some 
extent qualitative) element to this problem: rather than comparing a single 
fuzzer to a set of competing fuzzers across benchmarks, evaluating meta-fuzzing 
methods invovles evaluating a \emph{set} of fuzzers $F_1 \ldots F_n$ against 
$F'_1 \ldots F'_n$ (where some meta-fuzzing method has been applied), and 
determining the degree of improvement or degradation in effectiveness in each 
case.  This alone would make support for benchmarking and comparing fuzzers an 
important feature of any framework for meta-fuzzing research and development.  
However, as noted above, future ensemble methods are likely to also make use of 
fuzzer evaluations to allocate resources.  Therefore a major thrust of this 
effort is to adapt the aspects of the framework that support ensemble fuzzing 
to support measurements of fuzzer effectiveness as well.  This feature of 
course has applications beyond meta-fuzzing methods; traditional fuzzer 
evaluations can also be expected to benefit from a systematic, 
multi-measurement framework for running a set of fuzzers on multiple benchmarks.

\subsection{Overview of Proposed Work}

This project proposes, first, to create a reliable infrastructure for executing 
multiple fuzzers on a set of targets and collecting data from these executions. 
 Such data includes generated inputs and detected bugs, of course, but also 
measurements of fuzzer effectiveness, e.g., code coverage of corpuses and 
incremental code coverage.  This infrastructure will be able to run locally for 
debugging and quick turnaround, and will support cloud deployment for 
large-scale experiments. Critically, this infrastructure will be designed to 
support ensemble fuzzing and complex ensemble strategies, from the ground up; 
support for exchange of generated inputs and shifts in resource allocation will 
be built-in, rather than added on, and a high-level declarative language for 
describing ensemble strategies will allow researchers to explore the space of 
ensemble methods.

A second key element of the proposed system is the development of tooling for 
expressing source and binary-level transformations to support meta-fuzzing 
methods that rely on modifying the target to be fuzzer.  Such methods may focus 
on making more behavior of a system visible to fuzzers within the paradigm of 
``source coverage'' currently supported by essentially all modern fuzzers, or 
on enhancing oracles to enable all fuzzers to detect larger classes of bugs, or 
on transformations that target the underlying logic and bottlenecks of fuzzing, 
such as the non-chronological exploration of branches enabled by using program 
mutants.  This thrust, again, will focus on providing a way for researchers and 
developers to easily express such transforms and deploy them across a large set 
of fuzzers.

Finally, a smaller thrust will focus on allowing researchers and developers to 
write custom mutators for fuzzers in a fuzzer-independent way, with support for 
automatically generating wrappers to run these ``universalized'' mutators 
inside the large set of fuzzers (including AFLPlusPlus and libFuzzer) that 
support custom mutators.


\subsection{Team's Qualifications}

PI Groce has long-standing experience in fuzzing and test generation research, 
including substantial practical experience as a lead for automated testing on 
NASA's Curiosity Mars Lander project.  He is the lead developer for multiple 
open source testing tools, including the DeepState unit fuzzing tool, which has 
over 750 stars.  

\subsection{Intellectual Merit and Broader Impacts}

\paragraph{Intellectual merit.} The proposed work will advance the
knowledge and understanding of software testing by enabling
researchers and developers to conduct scalable experiments in meta-fuzzing.  
Meta-fuzzing approaches rely on the development of theories and heuristics 
about the exploration of program behavior, or the optimal allocation of 
resources to fuzzing tools, that apply without reference to specific underlying 
fuzzing algorithms.  Such approaches therefore, in addition to providing 
practical tools for finding bugs in code, advance our knowledge of the behavior 
of software systems.

\paragraph{Broader impacts.} Advancing the effectiveness of fuzzing can improve 
software testing in general and
thus software quality, benefiting (directly or indirectly) all
segments of society that depend on software.  The grant
will also support several graduate and undergraduate students, giving them 
experience with
building software artifacts and performing empirical studies.


\section{Key Deficiencies of Existing Infrastructure}

To our knowledge, \emph{no} infrastructure exists for performing 
fuzzing-specific source or binary transformations of programs, or writing 
custom mutators that can be used by more than one fuzzer. \jdh{Is this really 
true?  afl-clang and zafl produce instrumentated binaries that work with most 
of the AFL/AFL++/hongfuzz family of fuzzers.  Further, the afl/afl++ family of 
fuzzers can share input queues across the family alowing for ensemble fuzzing.  
Since each member of the ensemble can fuzz a different binary, it's very close 
to meta-fuzzing.  I get what you're saying, but I think it might be an 
over-claim.}  These aspects of meta-fuzzing fit into the picture of a promising 
research area with, essentially, no available tooling beyond individual 
fuzzers, outside the area of ensemble fuzzing.

In ensemble fuzzing, in addition to some deprecated projects, or ones of 
limited applicability, there is the PASTIS framework 
(https://github.com/quarkslab/pastis).  PASTIS was used to place first, tied 
with {\tt afltrusttrust} in the bug-detection category of the 2023 SBST fuzzing 
competition.  However, PASTIS at present only supports aflplusplus, Honggfuzz, 
and the symbolic execution tool TritonDSE.  Infrastructure to add more fuzzers, 
however, is present.  More importantly, PASTIS uses an extremely simple 
strategy:  it shares all seeds detected with all fuzzers and symbolic execution 
engines, and does no allocation of different resources to different fuzzers.  
Given the critical problem of fuzzer saturation, in settings with limited CPU 
resources (essentially all settings, given the number of available viable 
fuzzers and resource hunger of fuzzers), in the long run applying more 
sophisticated methods, e.g., from multi-armed bandit optimization (and in 
particular ``rotting bandit'' \cite{}) settings is critical for effective 
ensemble fuzzing.

Because PASTIS performs no resource allocation, it also relies on other 
infrastructure to perform any evaluation of the fuzzers being used in the 
ensemble, or of the whole strategy.  The SBST contest and some recent research 
has used FuzzBench, a service provided by Google.  However, FuzzBench is 
somewhat notoriously difficult to use and often produces failures, even for 
``core'' fuzzers such as aflplusplus, on some benchmarks, that prove difficult 
to diagnose or debug.  It is at best highly unclear how much support Google 
plans to devote to FuzzBench in the future.  The connection between fuzzer 
benchmarking and evaluation and ensemble fuzzing is, we note, clear enough that 
an (abandoned) attempt was made by the FuzzBench team to transform FuzzBench 
into an ensemble fuzzing tool (\url{}).   Other fuzzing benchmarks, such as 
MAGMA, are often tied to a specific set of benchmarks, with manual effort to 
make them fit into the framework, and so are not obviously suitable as a basis 
for ensemble fuzzing (and general evaluation purposes).  PASTIS, FuzzBench, and 
MAGMA all may provide useful insights into possible approaches to the core 
infrastructure of our ensemble fuzzing/fuzzer evaluation component, but none 
are, themselves, currently suitable for the purpose, and the benchmarking 
systems are even limited in usability for that purpose. 
\jdh{FuzzBench is also tied to the google cloud infrastructure, which is hard 
to stand up on it's own.  backending to Kubernetes, openstack, AWS, etc. would 
be nice.  None of the above tell you how or help you figure out how to use 
limited resources.  They all assume you just want to compare results given 
fixed resources.}


Source and binary transformation systems do exist, of course, but none are 
aimed at meta-fuzzing, which likely would strongly benefit from a 
Domain-Specific Language focused on making it easy to express the kinds of 
tranforms (in particular, introduction of ``mirror'' coverage structures to aid 
visibility) useful in meta-fuzzing.  Moreover, for binary instrumentation, some 
awareness of the instrumentation produced by various fuzzers is critical if 
transforms are to avoid modifying code that the fuzzer relies on working in a 
certain way.  In the extreme case, rather than simply producing incorrect 
instrumentation (e.g., coverage) results, such modifications could make a 
target completely unusable, by always triggering a crash of the target.
\jdh{I don't understand this paragraph.  The problem with (binary- or source-) 
instrumentation is that it has to match what the run-time component expects.  
This changes even within variants of the same fuzzer.  
We've had to change instrumentation styles between versions of afl++ to support 
variable map sizes, etc.  }
\section{Proposed Infrastructure}

\subsection{Thrust 1: A Foundation for Efficient Fuzzer Execution, 
Orchestration, and Evaluation}

The execution of large sets of fuzzers for purposes of evaluating 
(meta-)fuzzing approaches, or for less-explored but promising research such as 
determining which kinds of fuzzers work best for broad classes of fuzzing 
targets (e.g., compilers, media parsers, data structures, etc.) is essentially 
the same core task as that required for sophisticated ensemble fuzzing.  The 
only real difference is that a benchmarking run 1) runs all the fuzzers with 
the same resource allocation and 2) there is no sharing of inputs between 
fuzzers.  That is, fuzzer benchmarking is a simplified, restricted, version of 
what is needed for a successful platform for implementing ensemble fuzzing.  
The core needs are the same: ease of adding new fuzzers and including new 
targets, ease of deployment to both local environments (for easy debugging or 
use in developer ``unit fuzzing'') and the cloud (for large-scale fuzzing or 
fuzzer evaluation), and ability to produce easily interpretable, statistically 
usable, results (for both human consumption and resource allocation).

[UVA can you talk more about how to go about this thrust?]

\subsection{Thrust 2: Expressive Source and Binary-Level Transformations to 
Enable Meta-Fuzzing}

The key task in this thrust is twofold.  For both source and binary transforms, 
identifying and implementing (likely in the form of a DSL) the most important 
transforms common to meta-fuzzing algorithm is important.  Forcing users to 
construct complex ways to express that some dynamic data's value or behavior 
should be made visible as a static coverage construct in code for each such 
transform, for example, is both likely to frustrate exploration of such 
techniques and result in buggy implementations.  Secondarily, for binary 
transforms, an effort to identify and avoid changing fuzzer-introduced 
instrumentation code is critical.  This, as with custom mutator, wil require 
in-depth examination of the internals of popular fuzzers.  We also hope to 
propose standards for tools in future to ``watermark'' fuzzer instrumentation 
that should not be changed by meta-fuzzing transforms.

This thrust will be divided into two sub-thrusts, one for source-level and one 
for binary-level transformations, with PI Groce coordinating work to determine 
if common DSL elements exist, i.e., transforms that can be applied at either 
level.

\subsection{Thrust 3: Universalized Custom Mutators}

This thrust is concerned with making it possible to write custom mutators for 
multiple fuzzers in a single format, with a wrapper (or source transform) 
specializing this to individual fuzzers.

As a first task in this thrust, we propose to re-implement the compiler-fuzzing 
tool developed by PIs Groce and Le Goues as a custom mutator.  By focusing on a 
practical task, we will be able to best perceive difficulties and needs of 
custom mutator implementors, and can produce a library of useful byte and 
code-level mutators to be applied by users.

\subsection{Thrust 4:  Domain-Specific Fuzzers (Cross-Cutting)}

\section{Enabled Research Opportunities and Projects}

\section{Project Organization Plan}

%Awardee institution(s) commitment to operating and maintaining the
%infrastructure for its estimated useful life; and
%
%Detailed project management plan, with timeline, to create and deploy
%the new or enhance the existing research infrastructure.
\label{sec:plan}


Table~\ref{tab:plan} presents the high-level organization and timeline for the 
proposed
work. For each thrust or sub-thrust, one PI will lead the thrust or sub-thrust.


All implementation work will be coodinated by using a set of shared, public, 
GitHub repositories.    We plan to start our
infrastructure construction during summer when students are
not busy with courses and can focus on
building (and scaling) solid tooling.  The major thrusts can all operate in 
parallel, though fuzzer benchmarking is useful for evaluating experimental 
implementations using the tools developed.


This project will fund one graduate student per PI each year
(four total).  We will
involve starting graduate students, each for 1-2 years: once they
improve the infrastructure and learn how to use it for their research,
they will move on to research projects, while new
students continue the infrastructure work.  Additionally, work on the much more 
complex fuzzer benchmarking infrastructure underlying will be largely performed 
by systems scientists at UVA, given the complexity of this tooling and need for 
extremely high quality, maintainable code, since it is likely to become an 
underpinning of much future research in fuzzing, beyond meta-fuzzing.


Some of the PIs are already actively collaborating.
PIs Groce and Le Goues...


% LocalWords:  timeline analyses RAs REU PIs Zhang Basecamp Groce Joda ASE FSE
% LocalWords:  GitHub BitBucket



\section{Results from Prior NSF Support}

\paragraph{PI Groce:}
The most relevant prior NSF support for PI Groce is CCF-
CCF-2129446, ``Feedback-Driven Mutation Testing for Any Language,'' with a 
total budget of \$500,000 from 9/2021 until 8/2024,
a collaborative proposal with PI Le Goues of Carnegie Mellon
University. {\bf Intellectual Merit:} This project
focuses on a synergistic approach for allowing developers
to improve testing by using mutation testing to identify
weaknesses in tests and to generate tests.  In its second
year,  this project has already resulted in five
publications~\cite{cc2022,seip2022,fuzzing22}. {\bf
  Broader
  Impact:}  Work from this project has already
resulted in the reporting and correction of mulitple bugs in software
systems, including widely used production compilers.

\newpage
%\pagenumbering{roman}
%\setcounter{page}{1} 
%\bibliographystyle{unsrt}
\bibliographystyle{plain}
\bibliography{bibliography}

\end{document}
