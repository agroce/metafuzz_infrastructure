\subsection{Thrust 1: A Foundation for Efficient Fuzzer Execution,
Orchestration, and Evaluation}

The execution of large sets of fuzzers for purposes of evaluating
(meta-)fuzzing approaches, or for less-explored but promising research such as
determining which kinds of fuzzers work best for broad classes of fuzzing
targets (e.g., compilers, media parsers, data structures, etc.) is essentially
the same core task as that required for sophisticated ensemble fuzzing.  The
only real difference is that a benchmarking run 1) runs all the fuzzers with
the same resource allocation and 2) there is no sharing of inputs between
fuzzers.  That is, fuzzer benchmarking is a simplified, restricted, version of
what is needed for a successful platform for implementing ensemble fuzzing.
The core needs are the same: ease of adding new fuzzers and including new
targets, ease of deployment to both local environments (for easy debugging or
use in developer ``unit fuzzing'') and the cloud (for large-scale fuzzing or
fuzzer evaluation), and ability to produce easily interpretable, statistically
usable, results (for both human consumption and resource allocation).

\jdh{UVA can you talk more about how to go about this thrust?}

fuzzers -- which fuzers to use, and how to extend infrastructure for new fuzzers.  This is really NAU?
benchmarks -- standard fuzzing benchmarks, plus ability to add custom benchmarks.
specification -- which backend to use, how to allocate resources, how a fuzzing campaign should look like, which mutators to use, which instrumentation to use.
deployment -- VM allocation, software installation, efficient (semi-)automatic assignment of fuzzers based on fuzz campaign mapping
cooperation -- how often to share, and what to share.  Sharing "architectures" (e.g., share on the same VM, vs. different VMs in same cluster vs VMs in different clusters vs VMs in different data centers)
results -- collecting results (baseline results + user-specified extensions), statistical analysis


