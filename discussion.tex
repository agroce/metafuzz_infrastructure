




%\pagenumbering{gobble}
%\newpage  
%\pagenumbering{arabic}
%\pagenumbering{gobble}

\section{Introduction}
\label{sec:intro}

Fuzzing \cite{fuzzoverview} is one of the most important recent advances in 
automated software testing.  Modern coverage-driven fuzzers, such as AFL or 
libFuzzer, generally work by maintaing a set (called a corpus) of potentially interesting 
inputs for the program being fuzzed (often referred to as the target).  The corpus may consist of a 
trivial input, or examples of real-world inputs (or test cases), initially.  
The fuzzer then selects one such input, modifies it in some fashion, and 
submits the input to an instrumented version of the program being fuzzed.  If 
the execution produces novel behavior (e.g., covers new code or takes a new 
path through already-covered code), the modified input is added to the set of 
likely-interesting inputs.   If the execution reveals a bug, of course, it is 
saved for inspection.   Fuzzers differ widely in their strategies for selecting 
among interesting inputs, modifying inputs, and determining what constitutes 
potentially interesting behavior worth further exploring, but this broad picture of the basic 
structure of modern, effective fuzzers, is largely applicable, even to fuzzers 
making use of machine learning or symbolic execution techniques.

This basic approach has proved extremely effective in finding bugs, and in the 
shape of OSSFuzz (supported by Google, the Core Infrastructure Initiative and 
the OpenSSF), is used to probe a large number of critical open source systems, 
finding over 8,900 vulnerabilities and 28,000 bugs across 850 projects, to date.

Fuzzing has unsurprisingly become a major topic of academic security and 
testing research, as a result of the obvious power and success of the basic 
technique.  However, most such research results in \emph{the development of a 
new fuzzer.}  The most frequently seen kind of research paper in the field of fuzzing
research evaluates a
novel fuzzer developed by the academic researchers against a set of well-known 
fuzzers and recently published academic fuzzers.  Such work often results in 
the availability of new, powerful fuzzers, of course.  However, in some cases 
the underlying idea is a relatively isolated concept, that ends up being 
embedded in some cases in a technically less-than-sophisticated fuzzer.  E.g., 
the first release of the highly successful Eclipser fuzzer was very successful, 
due to the power of the lightweight semi-symbolic method used to modify 
inputs in intelligent ways.  However, the research team acknowledged that their 
implementation of more mundane aspects of fuzzing was somewhat \emph{ad hoc}, and the 
second version of Eclipser moved to using the widely used AFL fuzzer to support 
most aspects of fuzzing other than the core innovation.  Moreover, when a 
fuzzing advance is not adopted by other fuzzer developers, it can
essentially end up abandoned as part of an outdated fuzzer.  Fuzzing advances only appear in new fuzzers 
if future fuzzer developers adopt them as standard practice.  There have 
been some attempts to overcome this problem. For example, 
FuzzFactory~\cite{fuzzfactory} is a meta-fork of AFL which enables researchers 
to instatiate and compose fuzzers with custom feedback functions by only 
instrumenting the target program. Another prominent example is AFLPlusPlus, a 
version of AFL incorporating many academic and industrial fuzzing advances, 
which is a notably effective fuzzer.  However, there remain many individual 
programs where some other fuzzer outperforms AFLPlusPlus, and in fact use of 
many different, even sub-optimal, fuzzers is likely required for truly 
effective fuzzing.

\emph{Meta-fuzzing} proposes another approach to improving fuzzing, by 
\emph{moving the fuzzing technique outside the fuzzer itself.}  The simplest 
such approaches to consider are ones based on altering the fuzzing target, 
which is of course common across fuzzers.  A simple example is the idea of 
decomposing comparisons in a program.  Usually, a fuzzer can only observe if a 
program takes a given branch or not, at the binary level.  Decomposition breaks 
down CMP instructions into individual bit-level comparisons (or some other 
appropriate structuring) so that not only whether a branch was taken is 
visible, but how close a branch was to being taken (analogous to a branch 
distance in search-based testing).  There are various alternative versions of 
the basic idea, in e.g., Steelix and libFuzzer.  These are usually implemented 
as modifications to the fuzzer's custom instrumentation.  However, the basic 
idea can also be implemented by transforming the source code of a program to 
expose the structure of a comparison.  The DeepState property-driven fuzzing 
tool does this for its own assertion implementations, as does FuzzFactory in 
its \texttt{CMP} domain.  Such a transform might take, e.g.:

\begin{code}

if (x == y)
\end{code}

\noindent where {\tt x} and {\tt y} are integer variables, and rewrite the code 
as:

\begin{code}
cmp\_x\_y = TRUE;
bool cmp\_bit\_0 = bit(x, 0) == bit(y, 0);
if (cmp\_bit\_0) \{cmp\_x\_y = FALSE;\}
bool cmp\_bit\_1 = bit(x, 1) == bit(y, 0);
if (cmp\_bit\_1) \{cmp\_x\_y = FALSE;\}
$\ldots$
if (cmp\_x\_y)
\end{code}

\noindent which will be highly inefficient, of course, but exposes to the 
fuzzer when it has come close to solving an equality check.  The advantage of such 
an approach is that it enables \emph{any fuzzer} to make use of the power of 
decomposition, even if the fuzzer developers were unaware such a technique 
existed. 

While this particular technique, for efficiency reasons, is likely best 
implemented inside a fuzzer's instrumentation pass, other meta-fuzzing 
techniques are truly universal.  E.g., in recent work, PIs Groce and Le Goues 
proposed fuzzing based on program mutants \cite{}, which modifies a fuzzing 
target in a large variety of ways.  Results show that this approach, by 
allowing a fuzzer to explore program branches in non-chronological order, can 
improve the performance of even state-of-the-art fuzzers such as AFLPlusPlus, 
as benchmarked by Google's FuzzBench.

A core tenet of meta-fuzzing is that the fuzzing tool need not be aware of what 
the program transformation is trying to achieve. For example, in the 
FuzzFactory framework~\cite{fuzzfactory}, co-PI Padhye demonstrated that new 
fuzzing applications---such as finding worst-case performance or 
memory-allocation bottlenecks, as well as targeting fuzzing towards specific 
program locations---can be instantiated by only transforming the target program 
and using an API to send domain-specific feedback from program execution in the 
form of key-value maps. Padhye also demonstrated that composing program 
transformations, achieved by combining key-value maps across multiple 
FuzzFatory domains (such as breaking down CMP operations and tracking 
\texttt{malloc}'d memory), can produce results that are better than applying 
each transformation individually.

\paragraph{Ensemble Fuzzing.}   Ensemble fuzzing \cite{chen2019enfuzz} is an 
approach that recognizes the need for
diverse methods for test generation, at least in the context of
fuzzing.   Inspired by ensemble methods in machine learning 
\cite{dietterich2002ensemble},
ensemble fuzzing runs multiple fuzzers, and uses inputs generated by
each fuzzer to ``feed'' other fuzzers, since different fuzzers have widely 
differing strengths and weaknesses. Ensemble fuzzing is, by nature, a 
meta-fuzzing approach, in that it uses individual fuzzers as ``black boxes'' 
that take in inputs and a program and produce novel inputs of interest.  
Ensemble fuzzing is an extremely promising approach, given that in most 
large-scale benchmarks of fuzzers, there are times when the best fuzzers 
overall perform poorly for some targets, and when the worst fuzzers perform 
well.  The future of fuzzing likely lies in ensemble methods, but the 
implementations proposed thus far tend to 1) not be supported long, and soon 
fail to work at all 2) and/or are limited to very simple naive strategies for 
allocating resources and sharing seeds.  This project aims to make it easy to 
experiment with \emph{sophisticated} ensemble fuzzing approaches, without 
worrying about the infrastructure of executing fuzzers or extracting feedback 
to use in resource allocation.

\paragraph{Target Transformations.}  While ensemble fuzzing is ``meta'' in that 
it uses multiple fuzzers largely as black boxes, another approach largely 
dispenses with direct interaction with fuzzers at all.  As the example above 
suggests, one fuzzer-agnostic way to change the behavior of a fuzzer is to 
change the fuzzing target itself.  This is most easily done at the source 
level, in essence producing a (slightly) different program to fuzz.  In some 
cases, though, binary transformation may be required because the
source for a program is simply not available.   Transformations may be used to make aspects of 
program behavior more visible as coverage, as in the simple example, or to 
improve the oracle for fuzzing, transforming more behaviors into fuzzer-visible 
crashes.  More ambitiously, as in the prior work of PIs Groce and Le Goues, a 
transform may change the behavior of fuzzing algorithms.  Fuzzing program 
mutants allows fuzzers to detect ways to cover branches that, without 
transformation, would not have been executed yet, improving fuzzer 
effectiveness dramatically in some cases.

\paragraph{Universalized Custom Mutators.}  One feature supported by several 
widely used fuzzers is the introduction of custom mutators:  user-written 
additional ways to modify inputs to generate new inputs.  While changing the 
mutation strategy of a particular fuzzer can be a highly fuzzer-specific 
method, focusing on some unusual coverage or ``magic byte identification'' 
element of the particular fuzzer, many custom mutators are naturally fuzzer 
agnostic.  For example, PIs Groce and Le Goues introduced custom mutators for 
fuzzing compilers that resulted in the detection of hundreds of subtle compiler 
bugs \cite{CC22}.  These were implemented in a standalone variant of the 
original AFL fuzzer, but would be more widely used and more effective if 
implemented as custom mutators for a variety of fuzzers.  We propose to support 
research along these lines by allowing users to write a single custom mutator 
in a generic byte-based framework, and provide wrappers to integrate such 
mutators into all fuzzers that support custom mutators (including many of the 
most popular and powerful fuzzers).
   
\paragraph{An Example Meta-Fuzzing Application.} The above summaries introduce 
broad classes of meta-fuzzing approaches, and we have briefly introduced some 
general-purpose meta-fuzzing techniques (e.g., the use of program mutants to 
explore branches non-chronologically).  Meta-fuzzing can also be used for more 
domain-specific purposes, however.  Consider the problem of testing machine 
learning (ML) algorithms in general, and specifically neural network 
implementations.  Using off-the-shelf fuzzers directly on ML systems is usually 
ineffective, because the behavior of the system is not primarily encoded in the 
source code of the system, and thus visible to code coverage instrumentation.  
Instead, \emph{data} in the form of a neural network, decision tree, etc. 
determines the system behavior.  The ``code'' to be explored is not visible to 
traditional compiler instrumentation.  A meta-fuzzing approach to this problem 
would be to define transformers that understand the nature of a machine 
learning implementation (the standard libraries used widely to implement ML 
systems) and use the data encodings to produce \emph{source mirrors} that take 
inputs to the ML algorithm and produce visible code coverage for a fuzzer.  One 
aspect of our general meta-fuzzing infrastructure, discussed in more detail 
below, is additionally to support domain-specific fuzzing that falls outside 
the locus of ``fuzzing Linux application binaries'' previously centered in 
fuzzing benchmarking.


\paragraph{The Problem of Evaluation.} Evaluating fuzzers is, even in the 
current common setting, where evaluation is usually of a single ``new'' fuzzer, 
a complex problem. Many evaluations in the literature are insufficient or even 
incorrect.  Evaluating meta-fuzzing adds a further quantitative (and to some 
extent qualitative) element to this problem: rather than comparing a single 
fuzzer to a set of competing fuzzers across benchmarks, evaluating meta-fuzzing 
methods invovles evaluating a \emph{set} of fuzzers $F_1 \ldots F_n$ against 
$F'_1 \ldots F'_n$ (where some meta-fuzzing method has been applied), and 
determining the degree of improvement or degradation in effectiveness in each 
case.  This alone would make support for benchmarking and comparing fuzzers an 
important feature of any framework for meta-fuzzing research and development.  
However, as noted above, future ensemble methods are likely to also make use of 
fuzzer evaluations to allocate resources.  Therefore a major thrust of this 
effort is to adapt the aspects of the framework that support ensemble fuzzing 
to support measurements of fuzzer effectiveness as well.  This feature of 
course has applications beyond meta-fuzzing methods; traditional fuzzer 
evaluations can also be expected to benefit from a systematic, 
multi-measurement framework for running a set of fuzzers on multiple benchmarks.

\subsection{Overview of Proposed Work}

This project proposes, first, to create a reliable infrastructure for executing 
multiple fuzzers on a set of targets and collecting data from these executions. 
 Such data includes generated inputs and detected bugs, of course, but also 
measurements of fuzzer effectiveness, e.g., code coverage of corpuses and 
incremental code coverage.  This infrastructure will be able to run locally for 
debugging and quick turnaround, and will support cloud deployment for 
large-scale experiments. Critically, this infrastructure will be designed to 
support ensemble fuzzing and complex ensemble strategies, from the ground up; 
support for exchange of generated inputs and shifts in resource allocation will 
be built-in, rather than added on, and a high-level declarative language for 
describing ensemble strategies will allow researchers to explore the space of 
ensemble methods.

A second key element of the proposed system is the development of tooling for 
expressing source and binary-level transformations to support meta-fuzzing 
methods that rely on modifying the target to be fuzzer.  Such methods may focus 
on making more behavior of a system visible to fuzzers within the paradigm of 
``source coverage'' currently supported by essentially all modern fuzzers, or 
on enhancing oracles to enable all fuzzers to detect larger classes of bugs, or 
on transformations that target the underlying logic and bottlenecks of fuzzing, 
such as the non-chronological exploration of branches enabled by using program 
mutants.  This thrust, again, will focus on providing a way for researchers and 
developers to easily express such transforms and deploy them across a large set 
of fuzzers.

Finally, a smaller thrust will focus on allowing researchers and developers to 
write custom mutators for fuzzers in a fuzzer-independent way, with support for 
automatically generating wrappers to run these ``universalized'' mutators 
inside the large set of fuzzers (including AFLPlusPlus and libFuzzer) that 
support custom mutators.


\subsection{Team's Qualifications}

PI Groce has long-standing experience in fuzzing and test generation research, 
including substantial practical experience as a lead for automated testing on 
NASA's Curiosity Mars Lander project.  He is the lead developer for multiple 
open source testing tools, including the DeepState unit fuzzing tool, which has 
over 750 stars.  

\subsection{Intellectual Merit and Broader Impacts}

\paragraph{Intellectual merit.} The proposed work will advance the
knowledge and understanding of software testing by enabling
researchers and developers to conduct scalable experiments in meta-fuzzing.  
Meta-fuzzing approaches rely on the development of theories and heuristics 
about the exploration of program behavior, or the optimal allocation of 
resources to fuzzing tools, that apply without reference to specific underlying 
fuzzing algorithms.  Such approaches therefore, in addition to providing 
practical tools for finding bugs in code, advance our knowledge of the behavior 
of software systems.

\paragraph{Broader impacts.} Advancing the effectiveness of fuzzing can improve 
software testing in general and
thus software quality, benefiting (directly or indirectly) all
segments of society that depend on software.  The grant
will also support several graduate and undergraduate students, giving them 
experience with
building software artifacts and performing empirical studies.


\section{Key Deficiencies of Existing Infrastructure}

To our knowledge, \emph{no} infrastructure exists for performing 
\emph{fuzzing-specific} source or binary transformations of programs, or writing 
custom mutators that can be used by more than one fuzzer.  While
fuzzers can share corpuses, and their are commonalities to
instrumentation formats across the AFL family, there is to our
knowledge no tool support for exploiting these commonalities (e.g.,
adding new instrumentations based on novel source patterns).  These aspects of meta-fuzzing fit into the picture of a promising 
research area with, essentially, no available tooling beyond individual 
fuzzers, outside the area of ensemble fuzzing.

In ensemble fuzzing, in addition to some deprecated projects, or ones of 
limited applicability, there is the PASTIS framework 
(https://github.com/quarkslab/pastis).  PASTIS was used to place first, tied 
with {\tt afltrusttrust} in the bug-detection category of the 2023 SBST fuzzing 
competition.  However, PASTIS at present only supports aflplusplus, Honggfuzz, 
and the symbolic execution tool TritonDSE.  Infrastructure to add more fuzzers, 
however, is present.  More importantly, PASTIS uses an extremely simple 
strategy:  it shares all seeds detected with all fuzzers and symbolic execution 
engines, and does no allocation of different resources to different fuzzers.  
Given the critical problem of fuzzer saturation, in settings with limited CPU 
resources (essentially all settings, given the number of available viable 
fuzzers and resource hunger of fuzzers), in the long run applying more 
sophisticated methods, e.g., from multi-armed bandit optimization (and in 
particular ``rotting bandit'' \cite{}) settings is critical for effective 
ensemble fuzzing.

Because PASTIS performs no resource allocation, it also relies on other 
infrastructure to perform any evaluation of the fuzzers being used in the 
ensemble, or of the whole strategy.  The SBST contest and some recent research 
has used FuzzBench, a service provided by Google.  However, FuzzBench is 
somewhat notoriously difficult to use and often produces failures, even for 
``core'' fuzzers such as aflplusplus, on some benchmarks, that prove difficult 
to diagnose or debug.  It is at best highly unclear how much support Google 
plans to devote to FuzzBench in the future.  FuzzBench is also deeply
tied to Google cloud infrastructure, which means that practical use of
the system is not feasible if Kubernetes, openstack, AWS, or another
cloud approach is better for a particular user.

The connection between fuzzer 
benchmarking and evaluation and ensemble fuzzing is, we note, clear enough that 
an (abandoned) attempt was made by the FuzzBench team to transform FuzzBench 
into an ensemble fuzzing tool (\url{}).   Other fuzzing benchmarks, such as 
MAGMA, are often tied to a specific set of benchmarks, with manual effort to 
make them fit into the framework, and so are not obviously suitable as a basis 
for ensemble fuzzing (and general evaluation purposes).  PASTIS, FuzzBench, and 
MAGMA all may provide useful insights into possible approaches to the core 
infrastructure of our ensemble fuzzing/fuzzer evaluation component, but none 
are, themselves, currently suitable for the purpose, and the benchmarking 
systems are even limited in usability for that purpose.   Their
inherent focus on comparing fuzzers given fixed resources is not
general-purpose enough to be suitable for allocating limited resources
on the fly, as they stand.


Source and binary transformation systems do exist, of course, but none are 
aimed at meta-fuzzing, which likely would strongly benefit from a 
Domain-Specific Language focused on making it easy to express the kinds of 
tranforms (in particular, introduction of ``mirror'' coverage structures to aid 
visibility) useful in meta-fuzzing.  Moreover, for binary instrumentation, some 
awareness of the instrumentation produced by various fuzzers is critical if 
transforms are to avoid modifying code that the fuzzer relies on working in a 
certain way.  In the extreme case, rather than simply producing incorrect 
instrumentation (e.g., coverage) results, such modifications could make a 
target completely unusable, by always triggering a crash of the target.
\jdh{I don't understand this paragraph.  The problem with (binary- or source-) 
instrumentation is that it has to match what the run-time component expects.  
This changes even within variants of the same fuzzer.  
We've had to change instrumentation styles between versions of afl++ to support 
variable map sizes, etc.  }
\adg{That's what I meant -- it needs to be aware of the fuzzers and
  map size configs etc. to do this at binary level; at source,
  presumably the instrumentation itself takes care of all that, given
  a transform from some code/data element to a ``fake'' coverage structure}

\section{Infrastructure Description}

\subsection{Fundamental infrastructure}

\input{thrust1}

\subsection{Tools, resources, and datasets}

\clg{FIXME: in general, this subsection is described in the call as ``Tools, resources, and 
datasets: describe ancillary resources to be developed and integrated into the infrastructure 
system;''. I therefore think that my placing all non-thrust 1 content into it is almost certainly
wrong.  Some maybe should go in ``fundamental infrastructure'', other parts in ``User services.''}
\clg{FIXME: This needs to include: ``Medium proposals should indicate items that 
will be developed by the initial award, along with a vision for possible tools that 
might be appropriate for future enhancements;" }

\subsubsection{Expressive Source and Binary-Level Transformations to 
Enable Meta-Fuzzing}

The key task in this thrust is twofold.  For both source and binary transforms, 
identifying and implementing (likely in the form of a DSL) the most important 
transforms common to meta-fuzzing algorithm is important.  Forcing users to 
construct complex ways to express that some dynamic data's value or behavior 
should be made visible as a static coverage construct in code for each such 
transform, for example, is both likely to frustrate exploration of such 
techniques and result in buggy implementations.  Secondarily, for binary 
transforms, an effort to identify and avoid changing fuzzer-introduced 
instrumentation code is critical.  This, as with custom mutator, wil require 
in-depth examination of the internals of popular fuzzers.  We also hope to 
propose standards for tools in future to ``watermark'' fuzzer instrumentation 
that should not be changed by meta-fuzzing transforms.

This thrust will be divided into two sub-thrusts, one for source-level and one 
for binary-level transformations, with PI Groce coordinating work to determine 
if common DSL elements exist, i.e., transforms that can be applied at either 
level.

\subsubsection{Universalized Custom Mutators}

This thrust is concerned with making it possible to write custom mutators for 
multiple fuzzers in a single format, with a wrapper (or source transform) 
specializing this to individual fuzzers.

As a first task in this thrust, we propose to re-implement the compiler-fuzzing 
tool developed by PIs Groce and Le Goues as a custom mutator.  By focusing on a 
practical task, we will be able to best perceive difficulties and needs of 
custom mutator implementors, and can produce a library of useful byte and 
code-level mutators to be applied by users.

\subsubsection{Domain-Specific Fuzzers (Cross-Cutting)}

\subsection{User services}
\label{sec:user-services}

\clg{from the call: ``describe services to be integrated into the infrastructure, including 
mechanisms by which researchers will gain access to the infrastructure;''}
\clg{I'm wondering if one of the subsections I initially grouped under 
tools/resources/datasets might fit better down here.}

\subsection{Community engagement}

\cut{
From the call: ``: describe how the community will be engaged in the design,
development, and management of the infrastructure, including plans for a
Community Advisory Board;''

Call says (not specifically referencing this subsection, but later), two things:
(1) ``Means by which infrastructure utilization and user satisfaction will be
evaluated and used to refine and improve subsequent infrastructure operations;"
and (2) ``Community plans to provide long-term sustainability of the
infrastructure;''

Claire thinks it might make sense to put that content IN this section, but
specifically label the paragraphs in question so they're easy to find.
}

Community engagement is critical to the long-term success and viability of any
developed research infrastructure.  Our project includes specific development
plans as well as plans for outreach activities to enhance community engagement.  
Note that we are dedicating a portion of the project's CloudBank allocation to
the community, to enable them to both try out and use the framework for their
purposes, which constitutes a portion of our community engagement strategy and
will contribute to building a community and sustaining the project longer term. 

\paragraph{Community Advisory Board.} We have budgeted to invite a small group
of senior stakeholders in the fuzzing and testing communities to serve as a
Community Advisory Board to the project.  We include budget for travel to an
annual meeting, as well as an honorarium. We plan to co-locate these meetings with
other relevant PI or workshop meetings to minimize extraneous travel and
inconvenience for the members of the Board.  In the first meeting, we will focus
on eliciting guidance for capability design to support long-term research needs,
ensuring relevance to the broader community.  In the second meeting, we will
present intermediate implementation and design, to elicit early expert feedback
and inform early revisions.  In the final meeting, we will focus on concretizing
longer term open problems to be supported by future community or funded efforts. 

\clg{I mention the letters of collaboration below. Note that they CANNOT be
identified as potential community Board members. Is this an OK spot, or elsewhere?}

\paragraph{Workshops.}  We have budgeted and plan for two workshops to
facilitate community engagement.  The first, in year 1 of the project, will be
small and focused, and likely invitation-only. The goal of the first workshop is
to ideate with community stakeholders, specifically focusing on requirements
elicitation and overall framework design, ensuring the needs of the community
are met.  John Regehr (University of Utah) and Marcel Boehme (Max Planck
Institute for Security \& Privacy) are two community members whose particular
expertise we will seek; see the letters of collaboration that accompany this
proposal.  We will additionally invite other community members of similar
standing and experience for this early phase of requirements elicitation.   The
second workshop, in year 3 of the project, will be larger, and open, to demonstrate the
framework to the broader community, elicit feedback, and educate the community
of the framework and its capabilities.  This second workshop will be co-located
at a major conference in Software Engineering or Security, depending on timing
and location, to ensure exposure to a broad audience of stakeholders in the
relevant research communities. \clg{Dumb question, what are the best
Fuzzing-specific workshops to name-check here?}

\paragraph{Evaluation and Improvement.} Our engagement with the Community
Advisory Board and the community via the planned workshops will provide one set
of avenues for eliciting feedback about the framework to inform ongoing
iteration.  \clg{TODO: more here, incorporating a reference to the section that
Anh is writing about metrics?}  The provision of cloud credits to the community
to use the framework will both build the community and enable means to elicit
and quickly iterate on community feedback over the course of project
development. 

\paragraph{Long-term Sustainability.} Our deployment strategy uses
well-understood open source strategies, namely providing access to the framework
via GitHub.  We will follow established mechanisms, integrated into GitHub's
platform, for release management, continuous integration, and bug reporting and
bug fixing, as well as for eliciting and accepting contributions from community
members via feature and Pull Requests. 

As described in Section~\ref{sec:user-services}, we will provide facilities to
deploy the framework to several cloud providers and dedicate a portion of the
project's CloudBank allocation to the community.  The provision of generic cloud
deployment strategies will support long term usage beyond the scope of this
project.

Additionally, we plan, in the third year of the project, to actively pursue
additional funding avenues for ensuring the continued success and development of
the infrastructure, including CIRC Enhancement proposals or via the NSF POSE
program. 

\subsection{Community outreach}

From the call: ``describe plans for ongoing outreach to develop a diverse user
community led by the Community Outreach Director (required for Grand proposals)
and the outreach team:''

\clg{The purpose of this subsection is a bit confusing --- the part I'm puzzled by
is that only Grand proposals need a director, but this subsection seems required for all proposals.  That said,
I don't understand why this subsection is required, or required to be different from the community 
engagement subsection, for non-Grand proposals.}

\section{Enabled Research Opportunities and Projects}

From the call, must include: ``Compelling new CISE research opportunities enabled by the proposed infrastructure, including a description of the steps taken to identify the research opportunities enabled by the infrastructure, as well as evidence that a diverse community of users plan to use the capabilities provided;
Examples of focused research projects or use cases that the infrastructure will enable, beyond the research of the PIs and co-PIs (note that the novelty and innovative aspects of the research must be evident, along with clear evidence that the proposed infrastructure is essential to moving CISE research frontiers forward);
Description of the CISE research community and sub-disciplines that will use and benefit from the infrastructure; evidence that there is community support for the infrastructure, such as preliminary community activities and/or plans for its use;''

\section{Qualifications of the Project Team}

From the call: ``Qualifications of the PI, co-PIs, and other members of the project team to 
manage the creation or enhancement and operations of the research infrastructure in support of its users;''
Seems worth its own section, makes it easy for reviewers to find.

\section{Project Management Plan}

%Awardee institution(s) commitment to operating and maintaining the
%infrastructure for its estimated useful life; and
%
%Detailed project management plan, with timeline, to create and deploy
%the new or enhance the existing research infrastructure.
\label{sec:plan}

\clg{ALEX: if you draw me a bad hand-drawn gantt chart I can turn it into grotesque but visually-appealing latex.}
Table~\ref{tab:plan} presents the high-level organization and timeline for the 
proposed
work. For each thrust or sub-thrust, one PI will lead the thrust or sub-thrust.

\clg{We also need to include ``Commitment to share resources, participate in CIRC Virtual Organization, 
and CIRC community PI meetings.'' somewhere in these 15 pages...this Section seems like an OK place to do it, I guess?}
All implementation work will be coodinated by using a set of shared, public, 
GitHub repositories.    We plan to start our
infrastructure construction during summer when students are
not busy with courses and can focus on
building (and scaling) solid tooling.  The major thrusts can all operate in 
parallel, though fuzzer benchmarking is useful for evaluating experimental 
implementations using the tools developed.


This project will fund one graduate student per PI each year
(four total).  We will
involve starting graduate students, each for 1-2 years: once they
improve the infrastructure and learn how to use it for their research,
they will move on to research projects, while new
students continue the infrastructure work.  Additionally, work on the much more 
complex fuzzer benchmarking infrastructure underlying will be largely performed 
by systems scientists at UVA, given the complexity of this tooling and need for 
extremely high quality, maintainable code, since it is likely to become an 
underpinning of much future research in fuzzing, beyond meta-fuzzing.


Some of the PIs are already actively collaborating.
PIs Groce and Le Goues...



\section{Results from Prior NSF Support}

\paragraph{PI Groce:}
The most relevant prior NSF support for PI Groce is CCF-
CCF-2129446, ``Feedback-Driven Mutation Testing for Any Language,'' with a 
total budget of \$500,000 from 9/2021 until 8/2024,
a collaborative proposal with PI Le Goues of Carnegie Mellon
University. {\bf Intellectual Merit:} This project
focuses on a synergistic approach for allowing developers
to improve testing by using mutation testing to identify
weaknesses in tests and to generate tests.  In its second
year,  this project has already resulted in five
publications~\cite{cc2022,seip2022,fuzzing22}.\clg{Alex, that's just 3. :-P} {\bf
  Broader
  Impact:}  Work from this project has already
resulted in the reporting and correction of multiple bugs in software
systems, including widely used production compilers.

\paragraph{PI Le Goues:}
The most relevant prior NSF support for PI Le Goues is CCF-
CCF-2129388, ``Feedback-Driven Mutation Testing for Any Language,'' with a 
total budget of \$500,000 from 9/2021 until 8/2024,
a collaborative proposal with PI Groce of Northern Arizona University. {\bf Intellectual Merit:} This project
focuses on a synergistic approach for allowing developers
to improve testing by using mutation testing to identify
weaknesses in tests and to generate tests.  In its second
year,  this project has already resulted in five
publications~\cite{cc2022,seip2022,fuzzing22}. {\bf
  Broader
  Impact:}  Work from this project has already
resulted in the reporting and correction of multiple bugs in software
systems, including widely used production compilers.

